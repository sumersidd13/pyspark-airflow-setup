# PySpark and Airflow Setup

## Overview

This repository provides scripts and configurations for setting up an environment with PySpark and Apache Airflow. It includes:

- A shell script for installing dependencies and setting up the environment.
- A PySpark script for transferring data between MySQL and Snowflake.
- An Apache Airflow DAG for scheduling and running the PySpark job.

## Installation

### Prerequisites

- Ubuntu-based system
- Python 3.8
- Access to a MySQL database
- Access to a Snowflake account

### Setup

1. **Clone the Repository**

   ```bash
   git clone https://github.com/sumersidd13/pyspark-airflow-setup.git
   cd pyspark-airflow-setup
